{
  "nbformat_minor": 1,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c6730eac0d06430a676017100723699e9750f97c"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8baedbcf9f849537014b587e9d2e9dbf63d69b42"
      },
      "source": [
        "I will try to use Latent Semantic Indexing (LSI) via Gensim along with Neural Networks (NNs) in Keras to build my solution to the challenge. Ancilliary processing will be undertaken via NumPy, Pandas, SciKit Learn, NLTK, as well as a few other standard Python modules.\n",
        "\n",
        "My approach is as follows:\n",
        "\n",
        "1. Reduce the mutations to a few manageable classes by leveraging manual observation, i.e. \"get a feel\" for the data.\n",
        "    a. Whenever possible, process the mutations as one-hot vectors or probability distribution over amino acid alphabets present. This covers mutations of the type [AminoAcid0][Position][AminoAcid1]. Wildcard and null values are allowed for amino acids. We will refer to these as Type0 mutations.\n",
        "    b. Accommodate other mutations in Type1. Divide Type1 into narrower sub-classes based on patterns observed. Use a one-hot vector scheme for the input.\n",
        "    \n",
        "2. Use the training and test sets to generate a Gensim dictionary, and the taining set to generate a corpus. For each training and test sample, create an LSI model and project onto the training set corpus.   \n",
        "\n",
        "3. Merge the data from steps 1 and 2 to create input for NN-based modelling.\n",
        "    a. Try a traditional multi-layer NN. \n",
        "    b. Try a convolutional NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "bef32c3ba1af4dfdb7b332c6b4f67c60d7f3cf7b"
      },
      "source": [
        "# Function Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "10b649d4b2cca6b6bb75b3a658d80d7b279f7570"
      },
      "source": [
        "Let us sart off by loading the essential libraries and objects. (I like to do this at the very beginning of my script whenever possible.)"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "7269bf8a099a73a62f5e7dbb15a0a4ce599c2a40"
      },
      "source": [
        "from sklearn import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import getcwd\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import gensim\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e3ab4d5c1625f304979cb582381c5dd565dee9da"
      },
      "source": [
        "Let us create some ancilliary objects for later use."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "f67c6d7e4c8958ae09fbd00b641d10ac1c07c30e"
      },
      "source": [
        "\"\"\" Ancilliae for Text-Parsing Functions Below \"\"\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_list = stopwords.words('english')\n",
        "p_stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9d164077dfb69268d477ee3ed7d6aa5037d105f5"
      },
      "source": [
        "We will also need a method to parse article tokens, i.e. use the textual data given to us."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "e7232692e94ddf8e72f82b62580dd66f2b410962"
      },
      "source": [
        "\"\"\" Tokeniser for Textual Data \"\"\"\n",
        "def generate_tokens(article):\n",
        "\traw_data = article.lower()\n",
        "\traw_data = unicode(raw_data, errors='ignore')\n",
        "\n",
        "\traw_tokens = tokenizer.tokenize(raw_data)\n",
        "\n",
        "\tstopped_tokens = [t for t in raw_tokens if t not in stop_list]\n",
        "\n",
        "\treturn [p_stemmer.stem(s) for s in stopped_tokens]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8cb54c1afc0d092745d9a3adc17fa87494cf3064"
      },
      "source": [
        "Here is a method to create a Gensim dictionary from a set of test and training articles"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "d0838eb4f575fb9e6405c185167b51bda7265ced"
      },
      "source": [
        "\"\"\" Gensim Data Dictionary from Training and Test Sets \"\"\"\n",
        "def create_save_full_dictionary(train_set, test_set):\n",
        "\tdictionary = gensim.corpora.Dictionary()\n",
        "\ti_train = 0\n",
        "\tfor article in train_set:\n",
        "\t\tparsed_tokens_list = []\n",
        "\t\tparsed_tokens = generate_tokens(article)\n",
        "\t\tparsed_tokens_list.append(parsed_tokens)\n",
        "\t\tdictionary.add_documents(parsed_tokens_list)\t\n",
        "\t\tprint \"# Diagnostic: Training sample \"+str(\"%04d\" %i_train)+\" added to dictionary\"\n",
        "\t\ti_train += 1\n",
        "\ti_test = 0\n",
        "\tfor article in test_set:\n",
        "\t\tparsed_tokens_list = []\n",
        "\t\tparsed_tokens = generate_tokens(article)\n",
        "\t\tparsed_tokens_list.append(parsed_tokens)\n",
        "\t\tdictionary.add_documents(parsed_tokens_list)\n",
        "\t\tprint \"# Diagnostic: Test sample \"+str(\"%04d\" %i_test)+\" added to dictionary\"\n",
        "\t\ti_test += 1\n",
        "\tdictionary.save(\"full_dictionary_lsi\")\n",
        "\treturn dictionary\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c5b4507983b9d7bb813cdb9162ad5eb4d3a08936"
      },
      "source": [
        "Once we have a dictionary, we will need a Gensim corpus for the articles we will handle."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "fea36a9d8ceeadd8f3d1af81ab2e515f761902fa"
      },
      "source": [
        "\"\"\" Gensim Corpus from Integrated Dictionary and Article \"\"\"\n",
        "def create_corpus(dictionary, article_array):\n",
        "\ti_article = 0\n",
        "\tfull_tokens_list = []\n",
        "\tfor article in article_array:\t\n",
        "\t\tparsed_tokens = generate_tokens(article)\n",
        "\t\tfull_tokens_list.append(parsed_tokens)\n",
        "\t\tprint \"# Diagnostic: Article \"+str(\"%04d\" %i_article)+\" parsed for addition to merged corpus\"\n",
        "\t\ti_article += 1\n",
        "\tcorpus = [dictionary.doc2bow([entry for article_tokens in full_tokens_list for entry in article_tokens])]\n",
        "\treturn corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d36a5ffd6e7bfbb83f18aea3a9d9e7f54f3cf5fa"
      },
      "source": [
        "We are ready to do some \"heavy lifting\" with the mutation data. The following function is meant to explore the various categories of mutations we can deduce by inspection from the data provided. We will leverage Python's RegEx processing module, re, and data manipulation features of Pandas."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "7ee5d9473de113b1ea63f2bd595d5064a3954aa6"
      },
      "source": [
        "\"\"\" Collating Mutation Data into \\'dict\\' Objects of Pre-Determined Types -- Using Manual Observations to Classify Mutations by RegEx Analysis \"\"\"\n",
        "def generate_mutation_types(variant_dataframe):\n",
        "\t\n",
        "\tvariant_dataframe[\"Simple Variant Status\"] = variant_dataframe.Variation.str.contains(r\"[A-Z]\\d{1,7}[A-Z]$\", case=True)\n",
        "\tvariant_dataframe[\"Multi-Source Variant Status\"] = variant_dataframe.Variation.str.contains(r\"[A-Z]{2,10}\\d{1,7}[A-Z]$\", case=True)\n",
        "\tvariant_dataframe[\"Degenerate Variant Status\"] = variant_dataframe.Variation.str.contains(r\"[A-Z]\\d{1,7}\\*$\", case=True)   \n",
        "\tvariant_dataframe[\"Null Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^null\\d{1,7}[A-Z]\", case=False)\n",
        "\tvariant_dataframe[\"Fusion Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^.*Fusion$\", case=False)\n",
        "\tvariant_dataframe[\"Splice Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^.*splice$\", case=False)\n",
        "\tvariant_dataframe[\"Trunc Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^.*trunc$\", case=False)\n",
        "\tvariant_dataframe[\"Del Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^.*del$\", case=False)\n",
        "\tvariant_dataframe[\"Dup Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^.*dup$\", case=False)\n",
        "\tvariant_dataframe[\"DelIns Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^.*delins.*\", case=False)\n",
        "\tvariant_dataframe[\"Ins Variant Status\"] = variant_dataframe.Variation.str.contains(r\"^.*ins.*\", case=False)\n",
        "\t\n",
        "\tsimple_changes = variant_dataframe\n",
        "\tsimple_changes = simple_changes[simple_changes[\"Simple Variant Status\"] == True]\n",
        "\tsimple_changes = simple_changes[simple_changes[\"Multi-Source Variant Status\"] == False][\"Variation\"]\n",
        "\tdegenerate_changes = variant_dataframe[variant_dataframe[\"Degenerate Variant Status\"] == True][\"Variation\"]\n",
        "\tnull_changes = variant_dataframe[variant_dataframe[\"Null Variant Status\"] == True][\"Variation\"]\n",
        "\tfusion_changes = variant_dataframe[variant_dataframe[\"Fusion Variant Status\"] == True][\"Variation\"]\n",
        "\tsplice_changes = variant_dataframe[variant_dataframe[\"Splice Variant Status\"] == True][\"Variation\"]\n",
        "\ttrunc_changes = variant_dataframe[variant_dataframe[\"Trunc Variant Status\"] == True][\"Variation\"]\n",
        "\tdel_changes = variant_dataframe[variant_dataframe[\"Del Variant Status\"] == True][\"Variation\"]\n",
        "\tdup_changes = variant_dataframe[variant_dataframe[\"Dup Variant Status\"] == True][\"Variation\"]\n",
        "\tdelins_changes = variant_dataframe[variant_dataframe[\"DelIns Variant Status\"] == True][\"Variation\"]\n",
        "\tins_changes = variant_dataframe\n",
        "\tins_changes = ins_changes[ins_changes[\"Ins Variant Status\"] == True]\n",
        "\tins_changes = ins_changes[ins_changes[\"DelIns Variant Status\"] == False][\"Variation\"]\n",
        "\tother_changes = variant_dataframe\n",
        "\tother_changes = other_changes[other_changes[\"Simple Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Degenerate Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Null Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Fusion Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Splice Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Trunc Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Del Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Dup Variant Status\"] == False]\n",
        "\tother_changes = other_changes[other_changes[\"Ins Variant Status\"] == False][\"Variation\"]\n",
        "\n",
        "\tchange_type0_dict = \t{\"simple\":set(simple_changes.values), \n",
        "#\t\t\t\t\"multi_source:\"set(multi_source_changes.values), \n",
        "\t\t\t\t\"degenerate\":set(degenerate_changes.values), \n",
        "\t\t\t\t\"null\":set(null_changes.values)}\n",
        "\tchange_type1_dict = \t{\"fusion\":set(fusion_changes.values), \n",
        "\t\t\t\t\"splice\":set(splice_changes.values), \n",
        "\t\t\t\t\"trunc\":set(trunc_changes.values), \n",
        "\t\t\t\t\"del\":set(del_changes.values), \n",
        "\t\t\t\t\"dup\":set(dup_changes.values), \n",
        "\t\t\t\t\"delins\":set(delins_changes.values), \n",
        "\t\t\t\t\"ins\":set(ins_changes.values), \n",
        "\t\t\t\t\"other\":set(other_changes.values)}\n",
        "\n",
        "\treturn change_type0_dict, change_type1_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d77b8fe8ca2970730b3ffd7d50eb6a1495670d2a"
      },
      "source": [
        "We will take a look at the results from the preceding function later. For now, let us continue to the next function definition. We will use this one to determine the amino acid alphabet to be used and the largest value of the position of the mutation."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "e1155854a6eb89a5136d87d0325e84a2769edf9b"
      },
      "source": [
        "\"\"\" Amino Acid Alphabet and Maximum Position Value for Mutaions of the Form [AminoAcidLetter0][PositionNumber][AminoAcidLetter1] (Type0) \"\"\"\n",
        "def type0_parameters(type0_data):\n",
        "\tamino_acid_letters = set([])\n",
        "\tsimple_changes_first = set([x[0].upper() for x in type0_data[\"simple\"]]) \n",
        "\tamino_acid_letters = amino_acid_letters.union(simple_changes_first)\n",
        "\tsimple_changes_last = set([x[len(x)-1].upper() for x in type0_data[\"simple\"]])\n",
        "\tamino_acid_letters = amino_acid_letters.union(simple_changes_last)\n",
        "\tnull_changes_last = set([x[len(x)-1].upper() for x in type0_data[\"null\"]])\n",
        "\tamino_acid_letters = amino_acid_letters.union(null_changes_last)\n",
        "\tdegenerate_changes_first = set([x[0].upper() for x in type0_data[\"degenerate\"]])\n",
        "\tamino_acid_letters = amino_acid_letters.union(degenerate_changes_first)\n",
        "\t\n",
        "\tsimple_changes_num = set([int(x[1:len(x)-1]) for x in type0_data[\"simple\"]])\n",
        "\tnull_changes_num = set([int(x[4:len(x)-1]) for x in type0_data[\"null\"]])\n",
        "\tdegenerate_changes_num = set([int(x[1:len(x)-1]) for x in type0_data[\"degenerate\"]])\n",
        "\n",
        "\tmax_num = max(max(simple_changes_num), max(null_changes_num), max(degenerate_changes_num))\n",
        "\tposition_normaliser = 10.0**np.ceil(np.log(max_num)/np.log(10))\n",
        "\n",
        "\treturn amino_acid_letters, position_normaliser\t\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6064357178f4d7eec08e73c2d298419232dc91c6"
      },
      "source": [
        "We need one more function to help with processing mutation types. This one takes a mutation and classifies it."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "844170a45fbc17d136e8444ce8327c331810a3b7"
      },
      "source": [
        "\"\"\" Search Function for Mutation Type Given Loaded \\'dict\\' Objects Containing Relevant Data \"\"\"\n",
        "def mutation_type(mutation, type0_data, type1_data):\n",
        "\tfor type0 in type0_data.keys():\n",
        "\t\tif mutation in type0_data[type0]:\n",
        "\t\t\treturn type0\n",
        "\tfor type1 in type1_data.keys():\n",
        "\t\tif mutation in type1_data[type1]:\n",
        "\t\t\treturn type1\n",
        "\treturn \"other\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c7b1811d3d108867144fff8d29b1bda355b4d4af"
      },
      "source": [
        "Now, we are ready to define our NN models. We will use two such models, one will be a traditional deep NN, the other a Convlutional NN (CNN). We will deal with flattened 1D input for each training/test sample: more on this in the last function to be defined."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "3a6b37baddd90dc91e7ff97c59af544da1396fca"
      },
      "source": [
        "\"\"\" Traditional Multi-Layer Deep Neural Network Model to be Used as Classifier \"\"\"\n",
        "def nn_model_traditional(num_classes=9, num_input_nodes=549, num_nodes_layer=512, dropout_value=0.2, num_hidden_layers=3):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(num_nodes_layer, input_dim=num_input_nodes, init='normal', activation='relu'))\n",
        "\thidden_layer_count = 0\n",
        "\twhile(hidden_layer_count<num_hidden_layers):\n",
        "\t\tmodel.add(Dropout(dropout_value))\n",
        "\t\tmodel.add(Dense(num_nodes_layer, init='normal', activation='relu'))\n",
        "\t\thidden_layer_count += 1\n",
        "\tmodel.add(Dense(num_classes, init='normal', activation=\"softmax\"))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "\"\"\" Convolutional Neural Network Model to be Used as Classifier \"\"\"\n",
        "def nn_model_convolutional(num_filters=50, len_filter=10, num_input_nodes=549, num_classes=9):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Convolution1D(nb_filter=num_filters, filter_length=len_filter, activation='relu', input_shape=(num_input_nodes,1)))\n",
        "\tmodel.add(MaxPooling1D())     # Downsample the output of convolution by 2X.\n",
        "\tmodel.add(Convolution1D(nb_filter=num_filters, filter_length=len_filter, activation='relu'))\n",
        "\tmodel.add(MaxPooling1D())\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(num_classes, init='normal', activation='softmax'))     # For binary classification, change the activation to 'sigmoid'    \t\n",
        "\tmodel.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "abff9810902b05aa9bb0c46ca65fe09759e67355"
      },
      "source": [
        "Last, but not least, we will have to package our inputs into something that our NN models can handle. \n",
        "\n",
        "There are a few components to this:\n",
        "    1. Type0 mutation informatiion: first (2*a0 + 1) elements of a row, where a0 is the number of amino acid alphabets deduced from the training and test sets.\n",
        "    2. Type1 mutation information: next n1 elements of the row, where n1 is the number of type1 mutations observed.\n",
        "    3. Gensim LSI model projection: last t elements, where t is the number of topics to be used to analyse the cumulative repository of textual information. "
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "d10dc1d664f27bc078c4ae3b46fc5607caacbcf8"
      },
      "source": [
        "\"\"\" Neural Network Input Data Matrix from Mutation and Textual Data Along with Relevant Gensim Objects \"\"\"\n",
        "\n",
        "\"\"\" \tEach row being of the form: [1-hot vector for type0 amino acid (a0-many values), normalised type0 position (1 value), 1-hot vector for type0 amino acid (a0-many values), 1-hot vector for type1 mutations (n1-many values), topic model projections from article (t-many values)] \n",
        "\tEach row having (a0 + 1 + a0 + n1 + t) elements for every data sample \"\"\"\n",
        "def generate_X(mutation_array, article_array, type0_data, type1_data, num_topics, dictionary, lsi_model):\n",
        "\t\n",
        "\tnum_data = mutation_array.shape[0]\n",
        "\tif (article_array.shape[0] != num_data):\n",
        "\t\tprint \"# Error [generate_X(...)]: mutation_array.shape[0] != article_array.shape[0]\"\n",
        "\t\treturn None\n",
        "\n",
        "\ttype0_amino_acids, type0_normaliser = type0_parameters(type0_data)\n",
        "\tnum_type1 = len(type1_data.keys())\n",
        "\tmutation_data_len = (2*len(type0_amino_acids)+1) + num_type1\n",
        "\n",
        "\ti_data = 0\n",
        "\tX_data = np.empty((article_array.shape[0], mutation_data_len + num_topics), dtype=float)\n",
        "\tfor article_text in article_array:\n",
        "\t\ttype0_first = np.zeros((1,len(type0_amino_acids)))\n",
        "\t\ttype0_loc = np.zeros((1,1))\n",
        "\t\ttype0_last = np.zeros((1,len(type0_amino_acids)))\n",
        "\t\ttype1_vec = np.zeros(num_type1)\n",
        "\n",
        "\t\tmutation = mutation_array[i_data]\n",
        "\n",
        "\t\tchange_type = mutation_type(mutation, type0_data, type1_data)\n",
        "\n",
        "\t\tif(change_type == \"simple\"):\t# Type0 Mutation of Form [AminoAcid0][Position][AminoAcid1] \n",
        "\t\t\tamino_acid_first = mutation[0]\n",
        "\t\t\ttype0_first = np.array([int(x==amino_acid_first) for x in type0_amino_acids], dtype=float)\t# One-hot vector: AminoAcid0\n",
        "\t\t\tamino_acid_last = mutation[len(mutation)-1]\n",
        "\t\t\ttype0_last = np.array([int(x==amino_acid_last) for x in type0_amino_acids], dtype=float)\t# One hot vector: AminoAcid1\n",
        "\t\t\ttype0_loc[0,0] = int(mutation[1:len(mutation)-1]) / type0_normaliser\t# Normalised position\n",
        "\t\telif(change_type == \"null\"):\t# Type0 Mutation of Form null[Position][AminoAcid1] -- Zero-Hot Vector for \\'null\\' Value\n",
        "\t\t\tamino_acid_last = mutation[len(mutation)-1]\n",
        "\t\t\ttype0_last = np.array([int(x==amino_acid_last) for x in type0_amino_acids], dtype=float)\t# One hot vector: AminoAcid1\n",
        "\t\t\ttype0_loc[0,0] = int(mutation[4:len(mutation)-1]) / type0_normaliser\t# Normalised position\n",
        "\t\telif(change_type == \"degenerate\"):\t# Type0 Mutation of Form [AminoAcid0][Position]* -- Wildcard for AminoAcid1 => Equiprobable Vector Instead of One-Hot\n",
        "\t\t\tamino_acid_first = mutation[0]\t\n",
        "\t\t\ttype0_first = np.array([int(x==amino_acid_first) for x in type0_amino_acids], dtype=float)\t# One-hot vector: AminoAcid0\n",
        "\t\t\ttype0_last = np.ones((1,len(type0_amino_acids)))/float(len(type0_amino_acids))\t# Equiprobable vector: AminoAcid1; all values equally likely\n",
        "\t\t\ttype0_loc[0,0] = int(mutation[1:len(mutation)-1]) / type0_normaliser\t# Normalised position\n",
        "\t\telse:\t# Type1 Mutation\n",
        "\t\t\ttype1_vec = np.array([int(category==change_type) for category in type1_data.keys()], dtype=float)\n",
        "\t\t\n",
        "\t\t\"\"\" Mutation Data at Start of Row  \"\"\"\n",
        "\t\tX_data[i_data,0:len(type0_amino_acids)] = type0_first\n",
        "\t\tX_data[i_data,len(type0_amino_acids):len(type0_amino_acids)+1] = type0_loc\n",
        "\t\tX_data[i_data,len(type0_amino_acids)+1:2*len(type0_amino_acids)+1] = type0_last\n",
        "\t\tX_data[i_data,2*len(type0_amino_acids)+1:mutation_data_len] = type1_vec\n",
        "\t\t\n",
        "\t\t\"\"\" Projection of Textual Data onto Topic Model Basis Set \"\"\"\n",
        "\t\tarticle_tokens = generate_tokens(article_text)\t\t\n",
        "\t\tarticle_vec_bow = dictionary.doc2bow(article_tokens)\n",
        "\t\tarticle_vec_lsi = lsi_model[article_vec_bow]\n",
        "\t\tx_article = [v[1] for v in article_vec_lsi]\n",
        "\n",
        "\t\t\"\"\" Textual Projection Data at End of Row \"\"\"\n",
        "\t\tX_data[i_data, mutation_data_len:(mutation_data_len + num_topics)] = np.array(x_article)\n",
        "\n",
        "\t\ti_data += 1\n",
        "\n",
        "\treturn X_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2422ae8da1ed817eb36c19d64832bbdc8a883d92"
      },
      "source": [
        "# Main Execution Block for the Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "09ff2071ca2c90c93e9eac14ef0ea28b5a2a8840"
      },
      "source": [
        "Let us begin by loading the data provided to us by the competition organisers.\n",
        "\n",
        "We are in stage 2 of the competition. Hence, we use a combination of the stage 1 training set and a validated subset of the stage 1 test set as the stage 2 training set. Some data merging is called for.\n",
        "\n",
        "Of course, we have a brand new test set for stage 2."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "2e58bbd6337feccc0c5fe104a7ce47c184682dca"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "\ttopic_count = 500\n",
        "\n",
        "\t\"\"\" Reading Data from Training and Test Sets [Stage2_Training_Set = Stage1_Training_Set + Stage1_Validated_Test_Set] \"\"\"\n",
        "\ttrain_variant0 = pd.read_csv(getcwd()+\"/input/training_variants\")\n",
        "\ttrain_variant1 = pd.read_csv(getcwd()+\"/input/test_variants\")\n",
        "\tstage1_results_dataframe = pd.read_csv(getcwd()+\"/input/stage1_solution_filtered.csv\")\n",
        "\tstage1_indices = list(stage1_results_dataframe[\"ID\"])\n",
        "\ttrain_variant1 = train_variant1[train_variant1[\"ID\"].isin(stage1_indices)]\n",
        "\ttrain_variant1[\"ID\"] = train_variant1[\"ID\"] + train_variant0.values.shape[0]\t# Training set index offsets to avoid errors in mutation_type(...) due to concatenated data-frames \n",
        "\ttrain_variant = pd.concat([train_variant0, train_variant1])\n",
        "\ttest_variant = pd.read_csv(getcwd()+\"/input/stage2_test_variants.csv\")\n",
        "\ttrain_text0 = pd.read_csv(getcwd()+\"/input/training_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
        "\ttrain_text1 = pd.read_csv(getcwd()+\"/input/test_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
        "\ttrain_text1 = train_text1[train_text1[\"ID\"].isin(stage1_indices)]\n",
        "\ttrain_text1[\"ID\"] = train_text1[\"ID\"] + train_text0.values.shape[0]\t# Training set index offsets to avoid errors in mutation_type(...) due to concatenated data-frames \n",
        "\ttrain_text = pd.concat([train_text0, train_text1])\n",
        "\ttest_text = pd.read_csv(getcwd()+\"/input/stage2_test_text.csv\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
        "\n",
        "\t\"\"\" Combined Mutations for Both Training and Test Sets \"\"\"\t\t\n",
        "\tall_variant = pd.concat([train_variant, test_variant])\n",
        "\tall_genes = all_variant[\"Gene\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "03ccbe4f1777c523d9461d55c5aae3a0b6989b8b"
      },
      "source": [
        "Next, we parse the different types of mutations that are present in the training and test samples."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "8e09cf7fe3ed8e1b607115b7deaa35695a92835b"
      },
      "source": [
        "\t\"\"\" Compiling Mutation Types and Amino Acids \"\"\"\n",
        "\tchange_type0, change_type1 = generate_mutation_types(all_variant)\n",
        "\tnum_amino_acids_type0 = len(type0_parameters(change_type0)[0])\n",
        "\tnum_type1 = len(change_type1.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "44531a93d248fa06ac7f24832dfe06aac6382f73"
      },
      "source": [
        "Let us do an \"aside\" on iPython with the mutation data. [This is not part of the Python script we used to generate the submission.]"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "b141f245fe62b67fa7696092c5ab403d5d5dae13"
      },
      "source": [
        "change_type0.keys()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "6e2631cfa52de66738fdab6f150a589c75f4f39f"
      },
      "source": [
        "['simple', 'null', 'degenerate']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "00a2b6e4d58e082e8905b67baca30fac4a30b847"
      },
      "source": [
        "Let us explore the \"simple\" type0 mutations. We need to convert a set object into a list object to see the first 100 elements."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "f0aab1d86f445de6478285c373475c1af81e8b57"
      },
      "source": [
        "list(change_type0[\"simple\"])[:100]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "421118a5ff686364ee102011dd079d9b421de34b"
      },
      "source": [
        "['Y62C',\n",
        " 'L42R',\n",
        " 'N1297K',\n",
        " 'D399N',\n",
        " 'L858M',\n",
        " 'K1423E',\n",
        " 'E1586G',\n",
        " 'R263L',\n",
        " 'Y232C',\n",
        " 'V309L',\n",
        " 'H93Q',\n",
        " 'H93R',\n",
        " 'L858H',\n",
        " 'V564I',\n",
        " 'H93Y',\n",
        " 'H93D',\n",
        " 'E93D',\n",
        " 'L1195V',\n",
        " 'K4E',\n",
        " 'R2505P',\n",
        " 'R2505Q',\n",
        " 'E203K',\n",
        " 'E2856A',\n",
        " 'P380R',\n",
        " 'S786F',\n",
        " 'Y1463S',\n",
        " 'N581Y',\n",
        " 'G464E',\n",
        " 'M173I',\n",
        " 'S32I',\n",
        " 'N581T',\n",
        " 'N581S',\n",
        " 'T215K',\n",
        " 'H697Y',\n",
        " 'Y53H',\n",
        " 'L2396F',\n",
        " 'Y24C',\n",
        " 'N581D',\n",
        " 'S270L',\n",
        " 'H2074N',\n",
        " 'V1188L',\n",
        " 'L191H',\n",
        " 'H118P',\n",
        " 'A395D',\n",
        " 'V445M',\n",
        " 'D603N',\n",
        " 'G480W',\n",
        " 'F133L',\n",
        " 'D351H',\n",
        " 'R266Q',\n",
        " 'D603G',\n",
        " 'F460L',\n",
        " 'G17V',\n",
        " 'S860L',\n",
        " 'P86H',\n",
        " 'G1738R',\n",
        " 'G17E',\n",
        " 'V411L',\n",
        " 'T1324N',\n",
        " 'G17A',\n",
        " 'E216G',\n",
        " 'G776S',\n",
        " 'P124Q',\n",
        " 'G165V',\n",
        " 'P124S',\n",
        " 'G829R',\n",
        " 'G165R',\n",
        " 'R496C',\n",
        " 'I167N',\n",
        " 'P95S',\n",
        " 'S662G',\n",
        " 'R496H',\n",
        " 'G165E',\n",
        " 'R970C',\n",
        " 'P153H',\n",
        " 'D808N',\n",
        " 'G1743R',\n",
        " 'V118D',\n",
        " 'P106L',\n",
        " 'S562L',\n",
        " 'M504V',\n",
        " 'L387M',\n",
        " 'G885R',\n",
        " 'M1008I',\n",
        " 'C630Y',\n",
        " 'Y801H',\n",
        " 'E719G',\n",
        " 'E719K',\n",
        " 'R283Q',\n",
        " 'K509I',\n",
        " 'I391M',\n",
        " 'Q227L',\n",
        " 'V747L',\n",
        " 'E746G',\n",
        " 'D837N',\n",
        " 'P179L',\n",
        " 'E746K',\n",
        " 'E746Q',\n",
        " 'P1825A',\n",
        " 'R1459C']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6dd05daa5d94016836868da48387203dd006a8dd"
      },
      "source": [
        "We only look at the first 100 elements, because there are many mutations of this type... "
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "b197091f975c4a2c2f43fb17d7c05b2435113af2"
      },
      "source": [
        "len(change_type0[\"simple\"])"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "8794ac64f5275cf587d7bad32c5dea7b5e94be8d"
      },
      "source": [
        "3417"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "5796cf6519c4c5da020f141417fba0df4d518862"
      },
      "source": [
        "change_type0[\"null\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "ff81b3b0e19f7c60a9dbb2712062c93d3c4f6a63"
      },
      "source": [
        "{'null1313Y', 'null189Y', 'null262Q', 'null267R', 'null399R'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "4be77441a666c7bbcd91b48bc9ce832164592405"
      },
      "source": [
        "change_type0[\"degenerate\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "ea83be78f5b00e4986e09982ac1e50aee09390cc"
      },
      "source": [
        "{'A122*',\n",
        " 'E1322*',\n",
        " 'E14*',\n",
        " 'E160*',\n",
        " 'E1978*',\n",
        " 'E218*',\n",
        " 'E35*',\n",
        " 'E483*',\n",
        " 'E580*',\n",
        " 'E946*',\n",
        " 'Q233*',\n",
        " 'Q2416*',\n",
        " 'Q276*',\n",
        " 'Q337*',\n",
        " 'Q395*',\n",
        " 'Q429*',\n",
        " 'Q50*',\n",
        " 'Q531*',\n",
        " 'Q816*',\n",
        " 'R100*',\n",
        " 'R109*',\n",
        " 'R1093*',\n",
        " 'R1189*',\n",
        " 'R130*',\n",
        " 'R133*',\n",
        " 'R1464*',\n",
        " 'R162*',\n",
        " 'R174*',\n",
        " 'R177*',\n",
        " 'R226*',\n",
        " 'R2450*',\n",
        " 'R2505*',\n",
        " 'R304*',\n",
        " 'R315*',\n",
        " 'R335*',\n",
        " 'R348*',\n",
        " 'R383*',\n",
        " 'R389*',\n",
        " 'R421*',\n",
        " 'R487*',\n",
        " 'R621*',\n",
        " 'R659*',\n",
        " 'R680*',\n",
        " 'R711*',\n",
        " 'R798*',\n",
        " 'R802*',\n",
        " 'R922*',\n",
        " 'R978*',\n",
        " 'W279*',\n",
        " 'W345*',\n",
        " 'W714*',\n",
        " 'W719*',\n",
        " 'W802*',\n",
        " 'Y1003*',\n",
        " 'Y1045*',\n",
        " 'Y113*',\n",
        " 'Y1853*'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "c916d1fda39fce48148272adf085b88a3898ed92"
      },
      "source": [
        "change_type1.keys()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "24eec5f3232aae8c695f86c13216319702c8ad08"
      },
      "source": [
        "['fusion', 'other', 'del', 'splice', 'dup', 'delins', 'ins', 'trunc']"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "6ff237c6625621417d496c8f6cba816a602fa8de"
      },
      "source": [
        "change_type1[\"fusion\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "a7c97f31aabe5f39b3f7f54e3ce4b830df2341d4"
      },
      "source": [
        "{'ACPP-PIK3CB Fusion',\n",
        " 'AGK-BRAF Fusion',\n",
        " 'AKAP9-BRAF Fusion',\n",
        " 'ATF7IP-JAK2 Fusion',\n",
        " 'ATF7IP-PDGFRB Fusion',\n",
        " 'ATG7-RAF1 Fusion',\n",
        " 'BCAM-AKT2 Fusion',\n",
        " 'BCAN-NTRK1 Fusion',\n",
        " 'BCOR-CCNB3 Fusion',\n",
        " 'BCOR-RARA Fusion',\n",
        " 'BCR-ABL1 Fusion',\n",
        " 'BCR-FGFR1 Fusion',\n",
        " 'BCR-JAK2 Fusion',\n",
        " 'BCR-PDGFRA Fusion',\n",
        " 'BIN2-PDGFRB Fusion',\n",
        " 'BRD4-NUT Fusion',\n",
        " 'BTBD1-NTRK3 Fusion',\n",
        " 'CAD-ALK Fusion',\n",
        " 'CCDC6-ROS1 Fusion',\n",
        " 'CCND1-IGH Fusion',\n",
        " 'CD74-NTRK1 Fusion',\n",
        " 'CD74-ROS1 Fusion',\n",
        " 'CDK5RAP2-PDGFRA Fusion',\n",
        " 'CEP110-FGFR1 Fusion',\n",
        " 'CEP85L-PDGFRB Fusion',\n",
        " 'CEP85L-ROS1 Fusion',\n",
        " 'CHTOP-NTRK1 Fusion',\n",
        " 'CIC-DUX4 Fusion',\n",
        " 'CPEB1-NTRK3 Fusion',\n",
        " 'CUL1-BRAF Fusion',\n",
        " 'CUX1-FGFR1 Fusion',\n",
        " 'Delta-NTRK1 Fusion',\n",
        " 'EBF1-PDGFRB Fusion',\n",
        " 'EGFR-PURB Fusion',\n",
        " 'EGFR-RAD51 Fusion',\n",
        " 'EML4-ALK Fusion',\n",
        " 'EP300-MLL Fusion',\n",
        " 'EP300-MOZ Fusion',\n",
        " 'ERLIN2?FGFR1 Fusion',\n",
        " 'ESR1-CCDC170 Fusion',\n",
        " 'ESR1-YAP1 Fusion',\n",
        " 'ESRP1-RAF1 Fusion',\n",
        " 'ETV6-FLT3 Fusion',\n",
        " 'ETV6-NTRK3 Fusion',\n",
        " 'ETV6-PDGFRA Fusion',\n",
        " 'ETV6-PDGFRB Fusion',\n",
        " 'EWSR1-ATF1 Fusion',\n",
        " 'EWSR1-CREB1 Fusion',\n",
        " 'EWSR1-DDIT3 Fusion',\n",
        " 'EWSR1-ERG Fusion',\n",
        " 'EWSR1-ETV1 Fusion',\n",
        " 'EWSR1-ETV4 Fusion',\n",
        " 'EWSR1-FEV Fusion',\n",
        " 'EWSR1-FLI1 Fusion',\n",
        " 'EWSR1-NR4A3 Fusion',\n",
        " 'EWSR1-WT1 Fusion',\n",
        " 'EWSR1-YY1 Fusion',\n",
        " 'EZR-ERBB4 Fusion',\n",
        " 'EZR-ROS1 Fusion',\n",
        " 'FAM131B-BRAF Fusion',\n",
        " 'FGFR1-TACC1 Fusion',\n",
        " 'FGFR1OP1-FGFR1 Fusion',\n",
        " 'FGFR2-AHCYL1 Fusion',\n",
        " 'FGFR2-BICC1 Fusion',\n",
        " 'FGFR2-CCDC6 Fusion',\n",
        " 'FGFR2-CIT Fusion',\n",
        " 'FGFR2-FAM76A Fusion',\n",
        " 'FGFR2-KIAA1967 Fusion',\n",
        " 'FGFR2-MGEA5 Fusion',\n",
        " 'FGFR2-TACC3 Fusion',\n",
        " 'FGFR2?PPHLN1 Fusion',\n",
        " 'FGFR3 - BAIAP2L1 Fusion',\n",
        " 'FGFR3-TACC3 Fusion',\n",
        " 'FIG-ROS1 Fusion',\n",
        " 'FIP1L1-PDGFRA Fusion',\n",
        " 'FUS-ERG Fusion',\n",
        " 'GIT2-PDGFRB Fusion',\n",
        " 'GOLGA4-PDGFRB Fusion',\n",
        " 'GPIAP1-PDGFRB Fusion',\n",
        " 'HIP1-PDGFRB Fusion',\n",
        " 'HMGA2-RAD51B Fusion',\n",
        " 'IGH-BCL2 Fusion',\n",
        " 'IGH-FGFR3 Fusion',\n",
        " 'IGH-MYC Fusion',\n",
        " 'IGH-NKX2 Fusion',\n",
        " 'IGK-MYC Fusion',\n",
        " 'IGL-MYC Fusion',\n",
        " 'KANK1-PDGFRB Fusion',\n",
        " 'KDELR2-ROS1 Fusion',\n",
        " 'KDR-PDGFRA Fusion',\n",
        " 'KIAA1509-PDGFRB Fusion',\n",
        " 'KIAA1549-BRAF Fusion',\n",
        " 'KIF5B-ALK Fusion',\n",
        " 'KIF5B-PDGFRA Fusion',\n",
        " 'KIF5B-RET Fusion',\n",
        " 'LIMA1-ROS1 Fusion',\n",
        " 'LMNA-NTRK1 Fusion',\n",
        " 'LRIG3-ROS1 Fusion',\n",
        " 'MAGI3-AKT3 Fusion',\n",
        " 'MIR143-NOTCH1 Fusion',\n",
        " 'MKRN1-BRAF Fusion',\n",
        " 'MLL-TET1 Fusion',\n",
        " 'MPRIP-NTRK1 Fusion',\n",
        " 'MSN-ROS1 Fusion',\n",
        " 'NFASC-NTRK1 Fusion',\n",
        " 'NIN-PDGFRB Fusion',\n",
        " 'NPM-ALK Fusion',\n",
        " 'NSD1-NUP98 Fusion',\n",
        " 'PAPSS1-BRAF Fusion',\n",
        " 'PAX5-JAK2 Fusion',\n",
        " 'PAX8-PPAR? Fusion',\n",
        " 'PCM1-JAK2 Fusion',\n",
        " 'PDE4DIP-PDGFRB Fusion',\n",
        " 'PRKG2-PDGFRB Fusion',\n",
        " 'PTPRZ1-MET Fusion',\n",
        " 'PVT1-MYC Fusion',\n",
        " 'RABEP1-PDGFRB Fusion',\n",
        " 'RANBP1-ALK Fusion',\n",
        " 'RANBP2-ALK Fusion',\n",
        " 'RET-CCDC6 Fusion',\n",
        " 'ROS1-CD74 Fusion',\n",
        " 'RUNX1-EVI1 Fusion',\n",
        " 'RUNX1-RUNX1T1 Fusion',\n",
        " 'SDC4-ROS1 Fusion',\n",
        " 'SEC16A1-NOTCH1 Fusion',\n",
        " 'SLC34A2-ROS1 Fusion',\n",
        " 'SND1-BRAF Fusion',\n",
        " 'SPAG9-JAK2 Fusion',\n",
        " 'SPTBN1-PDGFRB Fusion',\n",
        " 'SRGAP3-RAF1 Fusion',\n",
        " 'SSBP2-JAK2 Fusion',\n",
        " 'STRN-ALK Fusion',\n",
        " 'STRN-PDGFRA Fusion',\n",
        " 'TEL-JAK2 Fusion',\n",
        " 'TEL-RUNX1 Fusion',\n",
        " 'TFG-NTRK1 Fusion',\n",
        " 'TFG-ROS1 Fusion',\n",
        " 'TMPRSS2-ERG Fusion',\n",
        " 'TMPRSS2-ETV1 Fusion',\n",
        " 'TMPRSS2-ETV4 Fusion',\n",
        " 'TMPRSS2-ETV5 Fusion',\n",
        " 'TP53BP1-PDGFRB Fusion',\n",
        " 'TPM3-NTRK1 Fusion',\n",
        " 'TPM3-ROS1 Fusion',\n",
        " 'TPR-NTRK1 Fusion',\n",
        " 'TRA-NKX2-1 Fusion',\n",
        " 'TRB-NKX2-1 Fusion',\n",
        " 'TRIM24-BRAF Fusion',\n",
        " 'WDR48-PDGFRB Fusion',\n",
        " 'YAP1-FAM118B Fusion',\n",
        " 'YAP1-MAMLD1 Fusion',\n",
        " 'YAP1-TFE3 Fusion',\n",
        " 'YWHAE-ROS1 Fusion',\n",
        " 'ZC3H7B-BCOR Fusion',\n",
        " 'ZNF198-FGFR1 Fusion'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "336047bd3cd36fafb6c319986cc79c83b68067c9"
      },
      "source": [
        "change_type1[\"del\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "91acdd62c0edcdac1af7cdf7d449ffe12e80b67c"
      },
      "source": [
        "{'385_418del',\n",
        " '533_534del',\n",
        " '534_536del',\n",
        " '550_592del',\n",
        " 'A750_E758del',\n",
        " 'A767_V769del',\n",
        " 'C456_N468del',\n",
        " 'C456_R481del',\n",
        " 'D289_D292del',\n",
        " 'D289del',\n",
        " 'D419del',\n",
        " 'D579del',\n",
        " 'D835del',\n",
        " 'D842_H845del',\n",
        " 'D842_M844del',\n",
        " 'E102_I103del',\n",
        " 'E1552del',\n",
        " 'E161del',\n",
        " 'E311_K312del',\n",
        " 'E439del',\n",
        " 'E554_I571del',\n",
        " 'E554_K558del',\n",
        " 'E554_V559del',\n",
        " 'E632_L633del',\n",
        " 'E746_A750del',\n",
        " 'G106_R108del',\n",
        " 'G469del',\n",
        " 'I32del',\n",
        " 'I33del',\n",
        " 'I563_L576del',\n",
        " 'I836del',\n",
        " 'I843_D846del',\n",
        " 'I843del',\n",
        " 'K1110del',\n",
        " 'K550_K558del',\n",
        " 'K550_W557del',\n",
        " 'K558_E562del',\n",
        " 'K558_V559del',\n",
        " 'K59del',\n",
        " 'K745_A750del',\n",
        " 'L283_D294del',\n",
        " 'L485_P490del',\n",
        " 'L485_Q494del',\n",
        " 'L576del',\n",
        " 'L57del',\n",
        " 'L747_A750del',\n",
        " 'L747_E749del',\n",
        " 'L747_P753del',\n",
        " 'L747_S752del',\n",
        " 'L747_T751del',\n",
        " 'L755_T759del',\n",
        " 'M199del',\n",
        " 'M1_E165DEL',\n",
        " 'M552_K558del',\n",
        " 'M552_W557del',\n",
        " 'N480del',\n",
        " 'N486_P490del',\n",
        " 'N542_E543del',\n",
        " 'N564_Y578del',\n",
        " 'P2415del',\n",
        " 'P447_L455del',\n",
        " 'P449_L455del',\n",
        " 'P490_Q494del',\n",
        " 'P551_E554del',\n",
        " 'P551_M552del',\n",
        " 'P551_V555del',\n",
        " 'P573_D579del',\n",
        " 'P577_D579del',\n",
        " 'Q347_A348del',\n",
        " 'Q556_K558del',\n",
        " 'Q56_V60del',\n",
        " 'Q579_L581del',\n",
        " 'Q58_E62del',\n",
        " 'S1297del',\n",
        " 'S459del',\n",
        " 'S45del',\n",
        " 'S752_I759del',\n",
        " 'T319del',\n",
        " 'T34_A289del',\n",
        " 'T488_P492del',\n",
        " 'T576del',\n",
        " 'V128del',\n",
        " 'V1578del',\n",
        " 'V1605del',\n",
        " 'V1688del',\n",
        " 'V241del',\n",
        " 'V422del',\n",
        " 'V555_L576del',\n",
        " 'V555_V559del',\n",
        " 'V559_G565del',\n",
        " 'V559_V560del',\n",
        " 'V559del',\n",
        " 'V560del',\n",
        " 'V569_L576del',\n",
        " 'W237_Y242del',\n",
        " 'W557_K558del',\n",
        " 'W557_V560del',\n",
        " 'W559_R560del',\n",
        " 'Y375_K455del',\n",
        " 'Y418_D419del',\n",
        " 'Y553_K558del',\n",
        " 'Y553_Q556del'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "ea5beca1736ae165dad3954a75cec0d06132aa05"
      },
      "source": [
        "change_type1[\"splice\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "b9a1d4dc0543afab0a220cc900a55aa9ea5b4089"
      },
      "source": [
        "{'596_619splice',\n",
        " '963_D1010splice',\n",
        " '981_1028splice',\n",
        " 'A113_splice',\n",
        " 'X1006_splice',\n",
        " 'X1007_splice',\n",
        " 'X1008_splice',\n",
        " 'X1009_splice',\n",
        " 'X434_splice',\n",
        " 'X475_splice',\n",
        " 'X582_splice',\n",
        " 'X963_splice'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "de70190d993a3dda116cfd020054f0a7a3e55de9"
      },
      "source": [
        "change_type1[\"dup\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "14efd141ac9c66cda2803b25c8a67db9f046538f"
      },
      "source": [
        "{'A502_Y503dup',\n",
        " 'A767_V769dup',\n",
        " 'D770_P772dup',\n",
        " 'G10dup',\n",
        " 'G778_P780dup',\n",
        " 'H773dup',\n",
        " 'N771_H773dup',\n",
        " 'P772_V774dup',\n",
        " 'S267_D273dup',\n",
        " 'S501_A502dup',\n",
        " 'S768_D770dup',\n",
        " 'T1151dup',\n",
        " 'T599dup',\n",
        " 'V218dup',\n",
        " 'Y772_A775dup'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "871fc37bcc52ec8ac90e0f3f9dbe420620bbda8f"
      },
      "source": [
        "change_type1[\"delins\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "5028b335e62cebd7d8f66b6808956a238f74713a"
      },
      "source": [
        "{'A750_E758delinsP',\n",
        " 'A859_L883delinsV',\n",
        " 'D842_I843delinsIM',\n",
        " 'E709_T710delinsD',\n",
        " 'E746_A750delinsQ',\n",
        " 'E746_S752delinsA',\n",
        " 'E746_S752delinsI',\n",
        " 'E746_S752delinsV',\n",
        " 'E746_T751delinsA',\n",
        " 'E746_T751delinsL',\n",
        " 'E746_T751delinsVA',\n",
        " 'F537_K539delinsL',\n",
        " 'G776delinsLC',\n",
        " 'G776delinsVC',\n",
        " 'H845_N848delinsP',\n",
        " 'I744_K745delinsKIPVAI',\n",
        " 'K459_S460delinsN',\n",
        " 'K550_V555delinsI',\n",
        " 'K558delinsNP',\n",
        " 'L485_P490delinsF',\n",
        " 'L485_P490delinsY',\n",
        " 'L747_A750delinsP',\n",
        " 'L747_P753delinsS',\n",
        " 'L747_T751delinsP',\n",
        " 'N198_F199delinsI',\n",
        " 'P551_W557delinsL',\n",
        " 'P577_W582delinsPYD',\n",
        " 'S566_E571delinsR',\n",
        " 'T417_D419delinsI',\n",
        " 'T417_D419delinsRG',\n",
        " 'T574_R588delinsL',\n",
        " 'V487_P492delinsA',\n",
        " 'V600_K601delinsE',\n",
        " 'V600delinsYM',\n",
        " 'W290_I291delinsC',\n",
        " 'W557_V559delinsC',\n",
        " 'Y568_L576delinsVN'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "7254a8c1107169c9b409037428f07bac45f897d8"
      },
      "source": [
        "change_type1[\"ins\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "0dfd749bb7b15bb40045ad79b32991dd5655bbcc"
      },
      "source": [
        "{'560_561insER',\n",
        " 'A11_G12insGA',\n",
        " 'A504_Y505ins',\n",
        " 'A763_Y764insFQEA',\n",
        " 'A775_G776insYVMA',\n",
        " 'C450_K451insMIEWMI',\n",
        " 'D600_L601insDFREYEYD',\n",
        " 'D600_L601insFREYEYD',\n",
        " 'D770_N771insD',\n",
        " 'D770_N771insNPG',\n",
        " 'D770_N771insSVD',\n",
        " 'D770_N771insVDSVDNP',\n",
        " 'DNA binding domain insertions',\n",
        " 'E598_Y599insDVDFREYE',\n",
        " 'E598_Y599insGLVQVTGSSDNEYFYVDFREYE',\n",
        " 'E612_F613insGYVDFREYEYDLKWEFRPRENLEF',\n",
        " 'E746_T751insIP',\n",
        " 'Exon 19 deletion/insertion',\n",
        " 'Exon 19 insertion',\n",
        " 'Exon 20 insertion',\n",
        " 'Exon 20 insertions',\n",
        " 'Exon 20 insertions/deletions',\n",
        " 'F594_R595insSDNEYFYVDF',\n",
        " 'G776_V777insYVMA',\n",
        " 'H773_V774insH',\n",
        " 'H773insLGNP',\n",
        " 'I559_D560insDKRMNS',\n",
        " 'K602_W603insYEYDLK',\n",
        " 'L601_K602insREYEYDL',\n",
        " 'L611_E612insCSSDNEYFYVDFREYEYDLKWEFPRENL',\n",
        " 'M774_A775insAYVM',\n",
        " 'P772_H773insYNP',\n",
        " 'Q58_Q59insL',\n",
        " 'R506_K507insVLR',\n",
        " 'S768_V769insVAS',\n",
        " 'S840_N841insGS',\n",
        " 'T244_I245insCPT',\n",
        " 'T574insTQLPYD',\n",
        " 'T599_V600insEAT',\n",
        " 'T599_V600insETT',\n",
        " 'T599_V600insV',\n",
        " 'T599insTT',\n",
        " 'V544_L545insAVLVLLVIVIISLI',\n",
        " 'V561_I562insER',\n",
        " 'V600D_K601insFGLAT',\n",
        " 'V769_D770insASV',\n",
        " 'V769_D770insGVV',\n",
        " 'W603_E604insDREYEYDLKW',\n",
        " 'Y599_D600insEYEYEYEY',\n",
        " 'Y599_D600insGLYVDFREYEY',\n",
        " 'Y599_D600insPAPQIMSTSTLISENMNIA',\n",
        " 'Y599_D600insSTDNEYFYVDFREYEY'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "29cb37a65683eed01ff2c865360770cd19742291"
      },
      "source": [
        "change_type1[\"trunc\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "e58a0e78c32754e817f66f68f9ab870104ca4d45"
      },
      "source": [
        "{'1_2009trunc',\n",
        " '2010_2471trunc',\n",
        " '256_286trunc',\n",
        " '422_605trunc',\n",
        " 'D286_L292trunc'}"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "234f3819741d813dbdead2eeb5edff69fe52df5e"
      },
      "source": [
        "change_type1[\"other\"]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "_uuid": "4b5dd887ceac8d55dfb1fe0689afd376a9b89cc9"
      },
      "source": [
        "{\"3' Deletion\",\n",
        " 'A603fs',\n",
        " 'AR-V7',\n",
        " 'ARv567es',\n",
        " 'Amplification',\n",
        " 'C1385',\n",
        " 'Copy Number Loss',\n",
        " 'DNA binding domain deletions',\n",
        " 'DNA binding domain missense mutations',\n",
        " 'DNMT3B7',\n",
        " 'Deletion',\n",
        " 'E23fs',\n",
        " 'EGFR-KDD',\n",
        " 'EGFRvII',\n",
        " 'EGFRvIII',\n",
        " 'EGFRvIV',\n",
        " 'EGFRvV',\n",
        " 'Epigenetic Silencing',\n",
        " 'Exon 1 mutations',\n",
        " 'Exon 11 deletions',\n",
        " 'Exon 11 mutations',\n",
        " 'Exon 13 deletion',\n",
        " 'Exon 19 deletion',\n",
        " 'Exon 2 mutations',\n",
        " 'Exon 9 mutations',\n",
        " 'F1088Lfs*5',\n",
        " 'F1088Sfs*2',\n",
        " 'F568fs',\n",
        " 'FLT3 internal tandem duplications',\n",
        " 'Fusions',\n",
        " 'Hypermethylation',\n",
        " 'K442Nfs*',\n",
        " 'L225LI',\n",
        " 'L232LI',\n",
        " 'L234fs',\n",
        " 'L370fs',\n",
        " 'MYC-nick',\n",
        " 'N1068fs*4',\n",
        " 'N1333Gfs*',\n",
        " 'Overexpression',\n",
        " 'P291Qfs*51',\n",
        " 'Promoter Hypermethylation',\n",
        " 'Promoter Mutations',\n",
        " 'Q1756fs',\n",
        " 'Q2405Rfs*17',\n",
        " 'R1627',\n",
        " 'R177Pfs*126',\n",
        " 'R574fs',\n",
        " 'S291fsX300',\n",
        " 'S453fs*',\n",
        " 'S70fsX93',\n",
        " 'S746fs',\n",
        " 'Single Nucleotide Polymorphism',\n",
        " 'T1481fs',\n",
        " 'T779fs',\n",
        " 'TGFBR1*6A',\n",
        " 'TRKAIII Splice Variant',\n",
        " 'Truncating Mutations',\n",
        " 'Truncating Mutations Upstream of Transactivation Domain',\n",
        " 'Truncating Mutations in the PEST Domain',\n",
        " 'V1075Yfs*2',\n",
        " 'Wildtype',\n",
        " 'p61BRAF'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c9fc15b67520e99d00593e7936de9b2730f6112c"
      },
      "source": [
        "We observe most of the data to contain type0 mutations of the \"simple\" sub-type. \n",
        "\n",
        "Type1 mutations have been modelled as \"low-resolution\" models. For example, we differentiate between \"trunc\" and \"delins\", but we do not delve into the details of either. On the other hand, we try to be more nuanced in our treatment of type0 mutations. \n",
        "\n",
        "Now, back to the original script's main execution block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7882ed820ea10d56d0b2ed50c988eac950e2174e"
      },
      "source": [
        "We gather the textual data from the articles and create a Gensim model. We also package the data into matrices suitable for processing via NN models built previously.\n",
        "\n",
        "The same spiel about stage 1 and stage 2 datasets (mentioned previously) applies for the data labels as well (dummy_y0, dummy_y1)."
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "2755bb5f329832db75b086ba2ad7859872c1f7d3"
      },
      "source": [
        "\t\"\"\" Textual Evidence Data \"\"\"\t\n",
        "\ttrain_articles = np.array(train_text[\"Text\"], dtype=str)\n",
        "\ttest_articles = np.array(test_text[\"Text\"], dtype=str)\n",
        "\n",
        "\t\"\"\" Gensim Objects and Models \"\"\"\n",
        "\tfull_dictionary = create_save_full_dictionary(train_articles, test_articles)\n",
        "\ttrain_corpus = create_corpus(full_dictionary, train_articles)\t# Only training set articles used as basis for topic model to search test data against\n",
        "\ttrain_lsi = gensim.models.LsiModel(train_corpus, id2word=full_dictionary, num_topics=topic_count)\n",
        "\n",
        "\t\"\"\" Mutation Datasets \"\"\"\n",
        "\ttrain_mutations = np.array(train_variant[\"Variation\"], dtype=str)\n",
        "\ttest_mutations = np.array(test_variant[\"Variation\"], dtype=str)\n",
        "\n",
        "\t\"\"\" Input Data for Neural Networks to be Used \"\"\"\n",
        "\tX_train = generate_X(train_mutations, train_articles, change_type0, change_type1, topic_count, full_dictionary, train_lsi)\n",
        "\tX_test = generate_X(test_mutations, test_articles, change_type0, change_type1, topic_count, full_dictionary, train_lsi)\n",
        "\n",
        "\t\"\"\" Saving Input Data for Future Use if Necessary \"\"\"\t\n",
        "\tX_train_dataframe = pd.DataFrame(X_train)\n",
        "\tX_train_dataframe.to_csv(\"X_train.csv\", index=False)\n",
        "\tX_test_dataframe = pd.DataFrame(X_test)\n",
        "\tX_test_dataframe.to_csv(\"X_test.csv\", index=False)\n",
        "\n",
        "\t\"\"\" Indices for Test Data Output \"\"\"\t\n",
        "\ttest_index = test_variant['ID'].values\n",
        "\n",
        "\t\"\"\" Labels (in One-Hot Vector Form) for Stage 1 Training Data and Validated Stage 1 Test Data \"\"\"\n",
        "\ty_train0 = train_variant0[\"Class\"].values\n",
        "\tencoder = LabelEncoder()\n",
        "\tencoder.fit(y_train0)\n",
        "\tencoded_y0 = encoder.transform(y_train0)\n",
        "\tdummy_y0 = np_utils.to_categorical(encoded_y0)\n",
        "\tdummy_y1 = stage1_results_dataframe.values[:,1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0a4973b93eb90be26a175a0bd1f57a81360e298f"
      },
      "source": [
        "Let us now check if the data labels (dummy_y0 and dummy_y1) for the stage 1 training set and the stage 1 validated test subset can be merged into an integrated training set for stage 2. If so, then, we can apply our NN models and generate solutions to the proposed problem. =)\n",
        "\n",
        "N.B. If dummy_y0 and dummy_y1 are mutually incompatible, we are left with an anti-climactic finale where nothing happens. =|"
      ]
    },
    {
      "execution_count": null,
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "_uuid": "44b65aa2a5654cf33bb84211f995d14ca4e4ce4c"
      },
      "source": [
        "\tif (dummy_y0.shape[1] == dummy_y1.shape[1]):\t# Neural Network Processing Only if Stage 1 Training Data Labels and Stage 1 Vaidated Test Data Labels Mutually Commpatible\n",
        "\t\t\"\"\" Consolidated Training Labels for Stage 2\"\"\"\n",
        "\t\tdummy_y = np.empty((dummy_y0.shape[0] + dummy_y1.shape[0], dummy_y1.shape[1]))\n",
        "\t\tdummy_y[0:dummy_y0.shape[0],:] = dummy_y0\n",
        "\t\tdummy_y[dummy_y0.shape[0]:dummy_y.shape[0],:] = dummy_y1\n",
        "\n",
        "\t\t\"\"\" Traditional Neural Network Model for Data \"\"\"\n",
        "\t\testimator_trad = KerasClassifier(build_fn=nn_model_traditional, epochs=10, batch_size=64, num_classes=9, num_input_nodes=topic_count+2*num_amino_acids_type0+1+num_type1, num_nodes_layer=512, dropout_value=0.2, num_hidden_layers=3)\n",
        "\t\testimator_trad.fit(X_train, dummy_y, validation_split=0.05)\n",
        "\t\ty_predicted_trad = estimator_trad.predict_proba(X_test)\n",
        "\n",
        "\t\t\"\"\" Submission File for Traditional Neural Network \"\"\"\n",
        "\t\tsubmission_trad = pd.DataFrame(y_predicted_trad)\n",
        "\t\tsubmission_trad['id'] = test_index\n",
        "\t\tsubmission_trad.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id']\n",
        "\t\tsubmission_trad.to_csv(\"submission_trad.csv\",index=False)\n",
        "\n",
        "\t\t\"\"\" Convolutional Neural Network Model for Data \"\"\"\n",
        "\t\testimator_conv = KerasClassifier(build_fn=nn_model_convolutional, epochs=10, batch_size=64, num_classes=9, num_input_nodes=topic_count+2*num_amino_acids_type0+1+num_type1, num_filters=50, len_filter=10)\n",
        "\t\testimator_conv.fit(np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)), dummy_y, validation_split=0.05)\n",
        "\t\ty_predicted_conv = estimator_conv.predict_proba(np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)))\n",
        "\n",
        "\t\t\"\"\" Submission File for Convolutional Neural Network \"\"\"\n",
        "\t\tsubmission_conv = pd.DataFrame(y_predicted_conv)\n",
        "\t\tsubmission_conv['id'] = test_index\n",
        "\t\tsubmission_conv.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id']\n",
        "\t\tsubmission_conv.to_csv(\"submission_conv.csv\",index=False)\n",
        "\telse:\t# Data Labels for the Two Separate Parts of Stage 2 Training Set Mutually Incompatible -- Not Possible to Train Neural Network\n",
        "\t\tprint \"# Error: Shape mismatch for \\'dummy_y\\' variable to be used for fitting models\""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "version": "2.7.13",
      "name": "python",
      "pygments_lexer": "ipython2",
      "file_extension": ".py",
      "nbconvert_exporter": "python",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}